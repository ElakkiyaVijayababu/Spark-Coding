{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark - Classification - Naive Bayes Classifier ML Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Summary\n",
    "- Task : Classification with binary or multiclass labels\n",
    "- Input : Label (binary or multiclass, 0-based indexed), feature vectors(discrete)\n",
    "- Smoothing : Additive smothing, default parameter is set to 1.0\n",
    "- Model type : Multinomial (default) or Bernouli. to use Bernouli, convert feature vectors to 0-1 vectors and set modelType to \"Bernouli\"\n",
    "- Assumptions:\n",
    "    - Independence between every pair of features\n",
    "    - Feature values are nonnegative, such as counts\n",
    "\n",
    "\n",
    "## Data Analysis Example\n",
    "- <a href=\"https://github.com/vincentarelbundock/Rdatasets/blob/master/csv/datasets/iris.csv\">Iris dataset</a>\n",
    "- Make a connection to spark cluster\n",
    "- Dataset Review\n",
    "- Load Data & Data preprocessing\n",
    "- Explore the data\n",
    "- Create a multiclass naive Bayes Classifier and Evaluation\n",
    "- Experimenting with Various Smoothing Parameters\n",
    "\n",
    "## Dataset Review\n",
    "The dataset contains 3 species of iris, there are Setosa, Versicolor and Virginica with 50 instances of each. in this example, we are going to try to predict the species from its features.\n",
    "\n",
    "Feature Information:\n",
    "1. Sepal Length in cm\n",
    "2. Sepal Width in cm\n",
    "3. Petal Length in cm\n",
    "4. Petal Width in cm\n",
    "\n",
    "Target and Label :\n",
    "- Species\n",
    "    - Setosa\n",
    "    - Versicolor\n",
    "    - Virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "spark_path = '/Users/pradmishra/Downloads/spark-2.4.0-bin-hadoop2.7'\n",
    "os.environ['SPARK_HOME']= spark_path\n",
    "os.environ['HADOOP_HOME']=spark_path\n",
    "sys.path.append(spark_path+'/bin')\n",
    "sys.path.append(spark_path+'/python')\n",
    "sys.path.append(spark_path+'/python/pyspark')\n",
    "sys.path.append(spark_path+'/python/lib')\n",
    "sys.path.append(spark_path+'/python/lib/pyspark.zip')\n",
    "sys.path.append(spark_path+'/python/lib/py4j-0.10.7-src.zip')\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries from python\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "from pyspark.ml.classification import NaiveBayes\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.0\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.6 (default, Jun 28 2018 11:07:29)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# Configure the necessary Spark environment\n",
    "import os\n",
    "import sys\n",
    "\n",
    "pyspark_submit_args = os.environ.get(\"PYSPARK_SUBMIT_ARGS\", \"\")\n",
    "if not \"pyspark-shell\" in pyspark_submit_args: pyspark_submit_args += \" pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, spark_home + \"/python\")\n",
    "\n",
    "# Add the py4j to the path.\n",
    "# You may need to change the version number to match your install\n",
    "sys.path.insert(0, os.path.join(spark_home, \"python/lib/py4j-0.10.7-src.zip\"))\n",
    "\n",
    "# Initialize PySpark\n",
    "exec(open(os.path.join(spark_home, \"python/pyspark/shell.py\")).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://pradeeptas-mbp-b72f:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Classification with Naive Bayes - Iris Datasets</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x120d12be0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get some context.\n",
    "#create a SparkContext and a SQLContext to use\n",
    "conf = SparkConf()\n",
    "#conf.setMaster(\"spark://sparklab-master:7077\")\n",
    "conf.setMaster(\"local[4]\")\n",
    "conf.setAppName(\"Spark Classification with Naive Bayes - Iris Datasets\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "sqlContext=SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"https://raw.githubusercontent.com/vincentarelbundock/Rdatasets/master/csv/datasets/iris.csv\"\n",
    "import pandas as pd\n",
    "df = pd.read_csv(data_file)\n",
    "del df['Unnamed: 0']\n",
    "df.columns = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth',\n",
    "       'Species']\n",
    "df.to_csv('iris.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we use csv format datasets and load the dataset with sqlCOntext format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=0, SepalLength=5.1, SepalWidth=3.5, PetalLength=1.4, PetalWidth=0.2, Species='setosa'),\n",
       " Row(_c0=1, SepalLength=4.9, SepalWidth=3.0, PetalLength=1.4, PetalWidth=0.2, Species='setosa'),\n",
       " Row(_c0=2, SepalLength=4.7, SepalWidth=3.2, PetalLength=1.3, PetalWidth=0.2, Species='setosa'),\n",
       " Row(_c0=3, SepalLength=4.6, SepalWidth=3.1, PetalLength=1.5, PetalWidth=0.2, Species='setosa'),\n",
       " Row(_c0=4, SepalLength=5.0, SepalWidth=3.6, PetalLength=1.4, PetalWidth=0.2, Species='setosa')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a dataframe\n",
    "#stored in a MYSQL database\n",
    "#data = sqlContext.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(data_file)\n",
    "#data.show()\n",
    "\n",
    "data = sqlContext.read.format('com.databricks.spark.csv').options(header='true', inferschema='true').load('iris.csv')\n",
    "data.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert target into numerical categories\n",
    "labelIndexer = StringIndexer(inputCol=\"Species\", outputCol=\"label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split row and just show SepalLength and Species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+\n",
      "|SepalLength|Species|\n",
      "+-----------+-------+\n",
      "|        5.1| setosa|\n",
      "|        4.9| setosa|\n",
      "|        4.7| setosa|\n",
      "|        4.6| setosa|\n",
      "|        5.0| setosa|\n",
      "|        5.4| setosa|\n",
      "|        4.6| setosa|\n",
      "|        5.0| setosa|\n",
      "|        4.4| setosa|\n",
      "|        4.9| setosa|\n",
      "|        5.4| setosa|\n",
      "|        4.8| setosa|\n",
      "|        4.8| setosa|\n",
      "|        4.3| setosa|\n",
      "|        5.8| setosa|\n",
      "|        5.7| setosa|\n",
      "|        5.4| setosa|\n",
      "|        5.1| setosa|\n",
      "|        5.7| setosa|\n",
      "|        5.1| setosa|\n",
      "+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"SepalLength\",\"Species\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=0, SepalLength=5.1, SepalWidth=3.5, PetalLength=1.4, PetalWidth=0.2, Species='setosa')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#take raw data from iris2 dataset.\n",
    "data.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model\n",
    "#Split the data into train and test\n",
    "# To proceed, we will first randomly split the dataset into training set (70%) and test set (30%).\n",
    "trainData, testData = data.randomSplit([0.7, 0.3], seed = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----------+-----------+----------+-------+\n",
      "|_c0|SepalLength|SepalWidth|PetalLength|PetalWidth|Species|\n",
      "+---+-----------+----------+-----------+----------+-------+\n",
      "|  0|        5.1|       3.5|        1.4|       0.2| setosa|\n",
      "|  1|        4.9|       3.0|        1.4|       0.2| setosa|\n",
      "|  2|        4.7|       3.2|        1.3|       0.2| setosa|\n",
      "|  3|        4.6|       3.1|        1.5|       0.2| setosa|\n",
      "|  5|        5.4|       3.9|        1.7|       0.4| setosa|\n",
      "|  6|        4.6|       3.4|        1.4|       0.3| setosa|\n",
      "| 10|        5.4|       3.7|        1.5|       0.2| setosa|\n",
      "| 11|        4.8|       3.4|        1.6|       0.2| setosa|\n",
      "| 12|        4.8|       3.0|        1.4|       0.1| setosa|\n",
      "| 13|        4.3|       3.0|        1.1|       0.1| setosa|\n",
      "+---+-----------+----------+-----------+----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trainData.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----------+-----------+----------+-------+\n",
      "|_c0|SepalLength|SepalWidth|PetalLength|PetalWidth|Species|\n",
      "+---+-----------+----------+-----------+----------+-------+\n",
      "|  4|        5.0|       3.6|        1.4|       0.2| setosa|\n",
      "|  7|        5.0|       3.4|        1.5|       0.2| setosa|\n",
      "|  8|        4.4|       2.9|        1.4|       0.2| setosa|\n",
      "|  9|        4.9|       3.1|        1.5|       0.1| setosa|\n",
      "| 15|        5.7|       4.4|        1.5|       0.4| setosa|\n",
      "| 18|        5.7|       3.8|        1.7|       0.3| setosa|\n",
      "| 19|        5.1|       3.8|        1.5|       0.3| setosa|\n",
      "| 21|        5.1|       3.7|        1.5|       0.4| setosa|\n",
      "| 25|        5.0|       3.0|        1.6|       0.2| setosa|\n",
      "| 26|        5.0|       3.4|        1.6|       0.4| setosa|\n",
      "+---+-----------+----------+-----------+----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "testData.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting Training Data : 103\n"
     ]
    }
   ],
   "source": [
    "print(\"Counting Training Data : {}\".format(trainData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting Test Data : 47\n"
     ]
    }
   ],
   "source": [
    "print(\"Counting Test Data : {}\".format(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=[\"SepalLength\", \"SepalWidth\", \"PetalLength\", \"PetalWidth\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\n",
      "+-----------------------+-------+\n",
      "|features               |clicked|\n",
      "+-----------------------+-------+\n",
      "|[18.0,1.0,0.0,10.0,0.5]|1.0    |\n",
      "+-----------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# example of Vector Assembler \n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "dataset = spark.createDataFrame([(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],[\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"hour\", \"mobile\", \"userFeatures\"],\n",
    "    outputCol=\"features\")\n",
    "\n",
    "output = assembler.transform(dataset)\n",
    "print(\"Assembled columns 'hour', 'mobile', 'userFeatures' to vector column 'features'\")\n",
    "output.select(\"features\", \"clicked\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train a NaiveBayes Model\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
    "\n",
    "#chain LabelIndexer, vecAssembler and NBmodel in a\n",
    "pipeline = Pipeline(stages=[labelIndexer, vecAssembler, nb])\n",
    "\n",
    "#Run stages in pipeline and train model\n",
    "model = pipeline.fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show the predictions\n",
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|  2.0|       2.0|[0.16435741351710...|\n",
      "|  2.0|       2.0|[0.18667884202571...|\n",
      "|  2.0|       2.0|[0.22632018815338...|\n",
      "|  2.0|       2.0|[0.19345806995097...|\n",
      "|  2.0|       2.0|[0.13852460169332...|\n",
      "|  2.0|       2.0|[0.17860399418640...|\n",
      "|  2.0|       2.0|[0.17324670108747...|\n",
      "|  2.0|       2.0|[0.19628177721965...|\n",
      "|  2.0|       2.0|[0.22520036100353...|\n",
      "|  2.0|       2.0|[0.23035185472860...|\n",
      "|  2.0|       2.0|[0.11991695747320...|\n",
      "|  2.0|       2.0|[0.21022032654016...|\n",
      "|  2.0|       2.0|[0.17317576418211...|\n",
      "|  2.0|       2.0|[0.17763221141908...|\n",
      "|  2.0|       2.0|[0.19497376441884...|\n",
      "|  2.0|       2.0|[0.22725681450445...|\n",
      "|  2.0|       2.0|[0.18439734839081...|\n",
      "|  0.0|       0.0|[0.50347518799337...|\n",
      "|  0.0|       0.0|[0.47461382207661...|\n",
      "|  0.0|       0.0|[0.48299790615630...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on testData so we can measure the accuracy of our model on new data\n",
    "predictions = model.transform(testData)\n",
    "print(\"Show the predictions\")\n",
    "predictions.select(\"label\",\"prediction\",\"probability\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accurary : 0.9361702127659575\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", \n",
    "                                              predictionCol=\"prediction\",\n",
    "                                              metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print (\"Model Accurary : {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'metricName: metric name in evaluation (f1|weightedPrecision|weightedRecall|accuracy) (default: f1, current: accuracy)'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.explainParam(\"metricName\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[13.,  2.,  0.],\n",
      "             [ 1., 14.,  0.],\n",
      "             [ 0.,  0., 17.]])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "# Create (prediction, label) pairs\n",
    "predictionAndLabel = predictions.select(\"prediction\", \"label\").rdd\n",
    "\n",
    "# Generate confusion matrix\n",
    "metrics = MulticlassMetrics(predictionAndLabel)\n",
    "print (metrics.confusionMatrix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Various Smoothing Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Create ParamGrid and Evaluator for Cross Validation\n",
    "paramGrid = ParamGridBuilder().addGrid(nb.smoothing, [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]).build()\n",
    "cvEvaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Cross-validation\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=cvEvaluator)\n",
    "cvModel = cv.fit(trainData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on testData. cvModel uses the bestModel.\n",
    "cvPredictions = cvModel.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+--------------------+\n",
      "|label|prediction|         probability|\n",
      "+-----+----------+--------------------+\n",
      "|  2.0|       2.0|[0.16457861889794...|\n",
      "|  2.0|       2.0|[0.18720068084740...|\n",
      "|  2.0|       2.0|[0.22738378398297...|\n",
      "|  2.0|       2.0|[0.19300467575986...|\n",
      "|  2.0|       2.0|[0.14017982615653...|\n",
      "|  2.0|       2.0|[0.18004482681025...|\n",
      "|  2.0|       2.0|[0.17460923737806...|\n",
      "|  2.0|       2.0|[0.19908569126593...|\n",
      "|  2.0|       2.0|[0.22624604045935...|\n",
      "|  2.0|       2.0|[0.23378946673763...|\n",
      "|  2.0|       2.0|[0.11963122871765...|\n",
      "|  2.0|       2.0|[0.21104925904871...|\n",
      "|  2.0|       2.0|[0.17342576291261...|\n",
      "|  2.0|       2.0|[0.17899828109252...|\n",
      "|  2.0|       2.0|[0.19560457704350...|\n",
      "|  2.0|       2.0|[0.22942331067819...|\n",
      "|  2.0|       2.0|[0.18485341056025...|\n",
      "|  0.0|       0.0|[0.50577482872555...|\n",
      "|  0.0|       0.0|[0.47919072043808...|\n",
      "|  0.0|       0.0|[0.48565715223632...|\n",
      "+-----+----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select results to view\n",
    "cvPredictions.select(\"label\", \"prediction\", \"probability\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9361702127659575"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate bestModel found from Cross Validation\n",
    "evaluator.evaluate(cvPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
